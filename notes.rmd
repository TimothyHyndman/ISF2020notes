---
title: "ISF 2020 notes"
author: "Timothy Hyndman"
date: "26/10/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    collapsed: false
    smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Session: Global Modelling for Forecasting

### Transfer learning schemes for global forecasting models using recurrent neural networks
#### Kasun Bandara
#### Abstract: 
Forecasting models that are trained across a set of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN) that require sufficient amounts of time series to estimate their numerous model parameters. Many time series databases however suffer from having only a limited number of time series. The transfer learning (TL) methodology has been proposed to overcome the limitation of training data by enabling knowledge transfer from models trained on a source task with sufficient amount of training data to a target task with few training instances. Although several studies have been conducted to forecast better under these circumstances, how to accurately forecast with GFMs in a situation with less data has not yet been thoroughly studied. To alleviate this problem, in this study, we investigate to what extent the TL methodology is useful, when using RNNs for GFM training. We propose a Residual RNN stacking architecture, supplemented by various TL schemes to overcome the challenges of time series sparsity, when developing GFMs. Using a comprehensive model training plan, we demonstrate the effectiveness of our proposed TL schemes for leveraging the capabilities of GFMs to forecast with a limited amount of time series. Furthermore, in a situation where time series data is not available to build a pre-trained model for knowledge transfer, we use GRATIS, a statistical generative model that artificially generates time series with diverse characteristics. In our evaluation on the kaggle web traffic forecasting challenge dataset, our proposed TL schemes are able to improve the baseline accuracy of GFM models, and outperform state-of-the-art univariate forecasting methods. The results also indicate that our approach can be operated to improve the forecasting accuracy, even in the presence of a single time series.

#### Notes:

- Global model: learns parameters from all series
- Don't perform so well if you don't have much data. How do you deal with this?

1. Augment the data by generating extra stuff
- GRATIS
- DBA
- STL + MBB

Gratis generates time series with diverse characterstics, the STL+MBB and DBA generate similar time series.

2. Can also use transfer knowledge (as well as? or as a different option?)

He used RNN with residual connections.

Stopped being able to follow from here...

Tested his method out on a bunch of data and compared to prophet, autoarima, and smooth::es. Results varied based on the data, but generally performed better than the benchmarks. Took much more computation than the statistical methods. MBB worked better than DBA for AusEnergy dataset - probably because it was smaller? Both DBA and MBB did better than GRATIS. 

Summary: Can still use RNNs without big data if you can augment your data (and maybe do some transfer learning?). The best methods are DBA and STL+MBB. Still depends on the data as to whether you can outperform statistical methods.


### Automatic feature-based forecast model averaging
#### Alexey Chernikov
#### Abstract:
Time series features are widely used in Big Data environments due to their capabilities in noise reduction, dimensionality reduction, and capturing relevant information of an object. Their use in time series classification achieves state-of-the-art accuracy and has become a de-facto standard. In forecasting, recent studies also demonstrate that time series features are able to contribute to achieve good results, e.g., in combination approaches with meta-learner based models. For example, the second place in the M4 Forecasting Competition was won by Feature-based Forecast Model Averaging (FFORMA), a model that exploits cross-series information in a meta-learner environment. In this model, the cross-series information is obtained using 42 statistical features that then serve as an input into the meta-learner. Despite the good result achieved with hand-crafted features, the approach has the potential limitations that hand-crafted features require domain knowledge and significant data-engineering work, which oftentimes is not feasible and may make the features not flexible enough to adapt to new datasets.

In this study we propose a novel approach of a meta-learner with time series features. We replace the hand-crafted features with automatically generated features of time series in FFORMA. In particular, We present a deep-learning method to extract time series features specifically for forecasting using a convolutional neural network. We also demonstrate the impact our features have on the prediction capabilities of meta-learner based models like FFORMA, and compare the results with a host of benchmarking methods, and in particular to methods based on conventional statistical features. We are able to obtain promising results in various application areas, in particular on the M4 dataset, even with significantly smaller amounts of features than the comparison methods.

#### Notes:
Feature based forecasting eg
- FFORMA
- FFORMS 
- etc

FFORMA uses TS features as a core component for combining models. Came second in M4 competition.
Uses handcrafted features (tsfeatures, catch22, tsfresh). Disadvantages:
- computationally intensive
- low discriminative power
- require domain knowledge
- feature engineering

Their research was into automating this stuff. Come up with an autoencoder. There was some problem with this, I think to do with not encoding based on the whole series, just using parts of it.

Static features: describe whole time series
Dynamic: describe time series in moment of time (look at a window)

Wanted to extract static features, but autoencoder only gives you dynamic features.

Use CNN to take dynamic features and turn into static features. Got 16 features which they then tested out on M4 data. Clustering on these features results in clusters with similar looking time series. 

Using these 'deep features' results in better performance that fforma with tsfeatures (and other benchmarks) in most benchmarks, others were close. This is important because FFORMA is based on M4 data to begin with (so automatic features does better than expert selection).

Also

- Used fewer features (42 vs 16)
- Not sensiitve to length of series
 

### Local model-agnostic interpretability in global time series forecasting
#### Dilini Rajapaksha
#### Abstract:
Forecasting models that are trained across a set of multiple related time series, known as Global Forecasting Models (GFM), have shown promising results in forecasting competitions and real-world applications while outperforming many typical univariate forecasting approaches. One aspect of the popularity of forecasting models such as ETS and ARIMA is their relative simplicity and interpretability which enables the trust of the users towards the forecast. The main drawback of more complex global models, such as globally trained neural networks (NN), is their lack of interpretability, especially localized to a particular time series. This reduces the trust and confidence of the stakeholders when making decisions based on the forecasts without being able to understand the predictions. To mitigate this problem, in this work, we propose a novel local model-agnostic interpretability approach to explain the forecasts given from forecasting models that are built using global modelling techniques such as NNs. The proposed framework is able to explain the forecast given from a global model for a particular time-series while considering the forecasting model as a black-box model, in a model-agnostic way. To achieve this we train simpler univariate models that are considered interpretable (e.g., ETS) on the time series which needs to be explained while minimizing the forecasting difference between the global black-box model and the statistical model. As the results of the proposed approach, the forecast produced by the black-box model can therewith be explained through components such as relevant lags, trend, seasonality, and potentially the influence of external factors, thus enabling the user to better understand the forecast.

#### Notes:
Generally simple models are more interpretable and less accurate, and complex models are more accuracte but less interpretable. So there is a trade off. To overcome this trade off, find an explanation of complex models. Enter "Local Interpretability". Common in Machien learning, uses local neighbourhood to explain particular predictions. 

In time-series forecasting we have local and global forecasting. Want to use local interpretability for global model. 

Global models have higher in-sample error and generalise better (i.e. doesn't over fit). 

Not sure exactly how the local explainers were generated.. something to do with fitting simple models (eg es, prophet etc) on the 'fit of the global model'?

Lost the flow of this talk after this.

### Addressing data heterogeneity in time series forecasting using global ensemble models
#### Rakshitha Godahewa
#### Abstract:
The performance of Neural Networks (NN) was poor compared to the traditional univariate forecasting models in many forecasting competitions in the past. But recently, global forecasting models have demonstrated that they have the potential to outperform traditional univariate forecasting models such as Exponential Smoothing State Space Model (ETS) and Autoregressive Integrated Moving Average (ARIMA) after contributing to the winning solution of the M4 forecasting competition. However, if a time series database contains heterogeneous series, the accuracy of the global forecasting models may degrade. This can happen especially through underfitting the data.
To bridge this gap, in this study, we focus on improving the accuracy of global forecasting models by addressing data heterogeneity using ensembling approaches. We present a series of ensemble forecasting models based on linear regression, Feedforward Neural Networks (FNN) and Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) cells. These models aggregate the predictions provided by a series of sub-models trained over different subgroups of time series within a dataset. The implemented ensemble models include clustering techniques that incorporate feature clustering, distance-based clustering and random clustering and the ensembling technique used for the winning solution of the M4 forecasting competition; the ensemble of specialists. We further combine the forecasts of our global models and traditional univariate forecasting models to improve the forecasting accuracy. Over six publicly available datasets, we show that ensembling a series of models trained over different parts of a particular dataset can improve the forecasting accuracy compared to non-ensemble models based on four evaluation metrics.

#### Notes:
- GFMs may not be so good is the time series are heterogeneous (diverse characteristics)
- Can train multiple GFMs for groups of similar time series (eg use clustering on series)

Can also use ensembling - aggregating predictions provided by a bunch of models.

2 main types - sequential (boosting), or parallel (bagging)

Also aggregating (averaging methods)

Considered three base methods (FFNN, RNN, and pooled regression). 
clustering methods (features, distance-based (DTW))
ensemble methods too

Conclusion: Clustered ensemble models work well for heterogeneous datasets


### Global models for time series forecasting: a simulation study
#### Hansika Hewamalage
#### Abstract:
Many state-of-the-art statistical techniques used for forecasting since the early days are univariate forecasting techniques which build a model per every series in isolation. However, with the recent advances in the infrastructure for storage, companies have collected enormous amounts of data over the years. Consequently, in the current context of Big Data, the nature of the forecasting problem has changed from predicting isolated series to predicting many (possibly related) series. The availability of large sets of time series opens up the opportunity to develop global models which simultaneously learn from many time series as opposed to traditional univariate techniques. Specifically in this respect, Recurrent Neural Networks (RNN) have demonstrated promising performance, e.g. in the M4 forecasting competition. Nevertheless, it still remains unclear under which circumstances global forecasting models can outperform the univariate benchmarks, especially along the dimensions of homogeneity/heterogeneity of series, complexity of patterns in series, complexity of models, and the lengths/number of series.

Our study investigates these issues, by simulating a number of datasets that have controllable characteristics. We perform experiments involving a number of global forecasting models including RNNs, pooled regression models and Feed-Forward Neural Networks within these simulated settings and compare their performance against standard statistical univariate benchmarks. In this manner, we seek to comprehend the behaviour of global forecasting models when encountered with datasets of specific characteristics. In our work, we introduce complexity into the time series in terms of the number of parameters of the Data Generating Process (DGP) as well as its non-linearity. The heterogeneity is introduced by mixing time series generated from several DGPs into a single dataset. Unlike the real-world datasets, the simulated environments render it possible to have complete control over the underlying datasets. Our initial experiments demonstrate that developing a global model across many short series helps better than developing many models on every individual short series on a homogeneous set of series. Also, as is to be expected, linear pooled regression models are competitive in fitting the series coming from a linear DGP. When it comes to more complex DGPs in terms of non-linearity, the complex modelling capabilities of the RNNs help further. Similarly, with respect to heterogeneous series with multiple simple DGPs as well, it is evident that RNNs can show superior performance as global models with their complex functionality. Thus, we are able to characterise trade-offs between the heterogeneity, complexity of the series and the forecasting performance of global models.

#### Notes:

Global models work well on 'related' time series. But what does 'related' mean in this context?

They simulate a number of time series, then run experiments on how related the time series have to be for global models to work well.

Generate data with AR models, seasonal AR models, Mackey-Glass Equation, SETAR model, chaotic logistic map, 
Forecast with RNN (lstm cells with peephole connections), FFNN, LGBM, pooled regression, univariate AR models

Did a bit of data preprocessing
- Normalisation (helpful esepcially for global models)
- Log transforms
- Moving window creation
- Window normalisation

Bunch of experimental scenarios, varying aspects of the series, and the models.

Different models do well in different scenarios - RNNs seem to do okay with Heterogenous series. LGBM good in some scenarios. PR good in others. AR and FFNN never the best. 

- PR good for simple homogeneous patterns. 
- More complex needs LGBMs and RNNs
- For heterogeneous series, use complex models again, even if individual series are simple. 
- RNNs, LGBMs competitive in variety of situations
- AR, PR make assumptions which might not hold
- LGBMs very good computationally, especially compared to RNNs
- Local models get more complex with more series, and computation time increases with nubmer of series too

### Why does joint forecasting of multiple time series work so well?
#### Pablo Montero-Manso
#### Abstract:
Organizations usually need to forecast multiple time series coming in groups, such as the demands for all products offered at one store, guest arrivals to all accommodation facilities at one city, etc. Local forecasting methods model each series individually, they fit a model to each product category or each accommodation facility. Global methods fit only one model for the whole group, pooling together the data for all time series. We can expect global methods to work better when the time series in the group are of similar nature, and local would be best when the group is heterogeneous. 

However, in the last few years global methods have been outperforming local methods even in some highly heterogeneous datasets. There is a lack of understanding of this phenomenon and the underlying principles that guide it. In particular, practitioners interested in the adoption of global methods face difficulties such as: When should we use global methods? How to group series? What constitutes similar series? What types of models are suitable for global methods? What sort of preprocessing do we need? What are the tradeoffs between local and global methods?

We will present theoretical and empirical results comparing global and local methods, focusing on two main ideas:

1) Global methods can be as accurate as local for any set of time series, including very heterogeneous sets. The strong assumption of global methods to fit a single model for all series in the set is not restrictive for forecasting.

2) The complexity of local methods grows with the size of the set. For example, even a simple local method such as exponential smoothing can be more complex than a global deep neural network when considering a large dataset.

We will show how these results can be applied in practice for designing new forecasting algorithms that improve over the state-of-the-art local methods.

#### Notes:
Local methods: statistical but also NN, trees etc when fit on single series
Global methods: 

We compare generalization gaps of local and global methods. The generalization gap increases with the number of parameters of the model, the more parameters, the worse it generalizes. (eg see occam's razor, akaike information criteria). 

IMagine we have dataset of 100 time series. Using local method like seasonal arima, each model needs about 2-5 parameters, plus differences. So around ~500 parameters for the whole dataset

Using a global method, if you can get to the same training error with fewer than 500 parameters, you will have better generalisation.

Broadly speaking, model complexity of local methods grows with number of series, but complexity of global methods is fixed. 

BUUTTTT
- multi-task learning not been that successful in general (really?)
- useless for really complex models (more params than samples (what does this mean))
- does not explain why we can fit really simple global models

##### Approximation capacity of global methods

why can we fit different time series with the same model.

Given a local method, there exists a global method that produces texactly the same predictions

- Does not mean that they are easy to find
- think about 'true' data generating processes for each time series
- set of time series does not matter
- it does not happen in general outside of time series

Example of how you can use AR model to exactly generate any degree 2 polynomial. So need just 1 global model with 3 parateters, vs each local model needing 3 parameters if you model each as degree 2 polynomial.

Show a similar thing with the M4 dataset

- Global methods are specific type of multi-task learners
- Can approximate as well as local methods
- IN practice can find very simple global models that fit large datasets
- in the absence of strong prior knowledge that favors one particular local method, global methods will be able to exploit the data much better.

# Session: Probabilistic Forecasting
### Optimal probabilistic forecasts: When do they work?
#### Ruben Loaiza-Maya
#### Abstract:
Proper scoring rules are used to assess the out-of-sample accuracy of probabilistic forecasts, with different scoring rules rewarding distinct aspects of forecast performance. Herein, we re-investigate the practice of using proper scoring rules to produce probabilistic forecasts that are `optimal' according to a given score, and assess when their out-of-sample accuracy is superior to alternative forecasts, according to that score. Particular attention is paid to relative predictive performance under misspecification of the predictive model. Using numerical illustrations, we document several novel findings within this paradigm that highlight the important interplay between the true data generating process, the assumed predictive model and the scoring rule. Only if a predictive model is sufficiently compatible with the true process to allow a particular score criterion to reward what it is designed to reward, will this approach to optimization reap benefits. Subject to this compatibility however, the superiority of the optimal forecast will be greater, the greater is the degree of misspecification. We explore these issues using a range of different scenarios, and using both artificially simulated and empirical data.

#### Notes:
Missed, due to previous session going over time

### Prediction of harvest date of shrimp in an aquaculture system
#### Ismael Sanchez
#### Abstract:
Shrimp is among the most valuable traded marine product in the world. Accurate prediction of both growing curve and optimal harvest date of shrimp in farms is, therefore, very important. Quality engineers can use it to detect a low growing of the shrimps. Managers can use it to negotiate prices in advance and organize the harvesting logistics. In this work, we build a forecasting system to provide the predictive density of the weight of the shrimp for the future weeks. Alternative approaches are compared, such us the use of growth models and nonparametric techniques. We apply the methodology to a large scale fish farm in northern Peru.
#### Notes:
Predicting shrimp growth in a pond - use similar trajectories in data to help build predictive models. Use predictive errors from similar trajectories or something like that I think

### Identification of factors and forecasting the possibility of human injuries due to an accident in colombo-katunayake expressway, Sri Lanka
#### Kushan Munasinghe
#### Abstract: 
Accidents are one of the main social problems in the world as well as developing countries like Sri Lanka, which cause damage or injuries unintentionally and unexpectedly. Sri Lanka's expressway system was launched in 2011 and currently has three major expressways: Southern Expressway, Colombo-Katunayake Expressway, and Outer-Circle Expressway. After the construction of expressways, many people opted for expressways on the basis of time, traffic, ease of driving, etc., rather than normal roads. The number of accidents on expressways has been on the rise in recent years. Nowadays, the rate of accident occurrence in Colombo-Katunayake Expressway is high compared to the other two Expressways and there was no previous research has been done in Sri Lanka regarding accidents in Colombo-Katunayake expressway. The aim of this study is to identify the factors that contribute to the accidents on the Colombo-Katunayake expressway and to develop appropriate regression and data mining models to forecast the possibility of human injuries due to an accident with a higher level of accuracy.

In this study, 704 total accident cases of Colombo-Katunayake expressway were considered during the period from 2013 to 2019. Pearson Chi-square, Logistic regression, Point biserial correlation and Kruskal–Wallis H tests were used to identify the association between the binary response variable (possibility of human injuries) and other influential variables. Finally, from selected predictor variables, seven variables: time category, driver’s age category, vehicle type, reason for accident, number of vehicles involved, cause for accident and rainfall were identified as influencing variables to the possibility of human injuries under 5% level of significance. Initially, the Binary Logistic Regression (BLR) model was performed to forecast the possibility of human injuries. Further, Classification tree (CART), Naïve Bayes classification algorithm, and Probabilistic neural network (PNN) were used in the study as data mining techniques to forecast the possibility of human injuries for the same datasets which were used to build and test the BLR model. Random Under-Sampling technique was used to overcome the class imbalance problem that persists in the data set considered in the study. Finally, Best performed models were found from each approach and the forecasting ability of these models was compared to come up with the best model to predict the possibility of human injuries.

The BLR model predicts the possibility of human injuries due to accidents with an accuracy of 70.7% and among data mining models, the PNN exhibits a higher overall prediction accuracy than classification tree and Naïve Bayes. The best PNN outperforms the best BLR model and finally, this model can be proposed as a better model to forecast the possibility of human injuries in Colombo-Katunayake expressway. The best performed PNN model contains two hidden layers, with 176 neurons in the pattern layer and two neurons in the output layer with 0.1 spread value. This model has the ability to forecast unseen data with 75.7% accuracy, 78.7% precision and 92.3% recall. The final model developed by this study can be used to implement safety improvements against traffic accidents in expressways of Sri Lanka.

#### Notes:
Didn't listen.


# Session: Hierarchical Forecasting I

### Forecast reconciliation: A geometric view with new insights on bias correction
#### George Athanasopoulos
#### Abstract:
A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular, a general framework is established nesting many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First, reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric. Second, the MinT method minimises expected loss for this same class of loss functions. Third, the geometric interpretation provides a new proof that forecast reconciliation using projections results in unbiased forecasts provided the initial base forecasts are also unbiased. Approaches for dealing with biased base forecasts are proposed. An extensive empirical study on Australian tourism flows demonstrates the theoretical results of the paper and shows that bias correction prior to reconciliation outperforms alternatives that only bias-correct or only reconcile forecasts.

#### Notes:

Generalised perspectives on forecast reconciliation. 

- Hierarchical time series defined by linear constraints (not just sums)
- Reconciliation via projection improves forecast accuracy w.r.t a certain class of loss functions
- 
- 

Can define time series as series which lie in coherent subspace. For example, 
$Y_T = Y_A + Y_B$ forms a plane with basis vectors (1, 1, 0) and (???). When we reconcile a forecast, we are mapping incoherent forecasts to the coherent subspace. Do this by pre-multiplying base forecasts by a nxn matrix that has $s$ (the subspace) as its image. 

So the most obvious is to use a Euclidean orthogonal projection. This guarentees that
- Base and reconciled forecasts are as close as possible
- Reconciled forecasts are closer to actuals than base forecasts

However, this measure is the sum of squared errors across ALL series. Consider differences in scales and predictability. May want to suggest different weights.

Using weights in the loss function you end up with oblique projections. You can then show that property two above still holds using a norm based on the weights.

MinT is a unique oblique projection that minimises the trace of your covariance matrix. There was a neat geometric explanation of how this minimises forecast errors of the reconciled forecasts.

Empirical results: Looked at australian tourism data. Use auto.arima() to generate 1 step ahead forecasts. Use 4 different projections, based on different loss functions. Each projection should be best for the different loss functions. The projection for the loss function results in errors being strictly better than base functions. MinT does best on average for everything though.

So can either
- Choose to guarantee that reconciled forecasts improve upon base forecasts
- find the reconciliation method that is best on average

Fantastic talk!!!


### Probabilistic Forecasts in Hierarchical Time Series
#### Anastasios Panagiotelis

#### Abstract:
We develop a framework for prediction or forecasting of multivariate data that follow some known linear constraints, such as the example where some variables are aggregates of others. For point prediction, an increasingly popular technique is reconciliation, whereby predictions or forecasts are made for all series (so called ‘base’ forecasts) and subsequently adjusted to ensure coherence with the constraints. This paper extends reconciliation from the setting of point prediction to probabilistic prediction. A novel definition of reconciliation is developed and used to construct densities and draw samples from a reconciled probabilistic prediction. In the elliptical case, it is proven that the true predictive distribution can be recovered from reconciliation even when the location and scale matrix of the base prediction are chosen arbitrarily. To find reconciliation weights, an objective function based on scoring rules is optimised. The energy and variogram scores are chosen since the log score is improper in the context of comparing unreconciled to reconciled forecasts, a result also proved in this paper. To account for the stochastic nature of the energy score, optimisation is achieved using stochastic gradient descent. The proposed algorithm is applied in a number of simulated scenarios and to the empirical example of forecasting electricity generation from different sources (e.g, renewable v non-renewable, coal, gas, wind solar etc.).

#### Notes:

https://anastasiospanagiotelis.netlify.app/talks/probreco/probreco#1

Can do the same definition for coherence, but on probabilities, instead of points. End up using some measure theory to talk about images, instead of pre-multiplying by a matrix, and pre-images instead of inversions.

Take the probability from the space (A), and assign that probability to the image on the subspace.

They proved:
- Reconciling a sample from base distribution, gives sample from reconciled distribution
- If base is elliptical, then reconciled is elliptical (eg normal)
- For any mu and sigma, there exists a d and G (a reconciliation) that recovers the true prediction distribution (but it's not feasible to find these! and won't always be a projection)

So need to find a way to find d and G (and not necessarily use a projection). 

We use a scoring function and optimise with respect to this scoring. Example
- log score. But doesn't satisfy some desired properties
- energy score
- variogram score

Got a little lost here... you use scoring function to find d and G...


### A geometry inspired hierarchical forecasting methodology
#### Nikos Kourentzes
#### Abstract:

Hierarchical forecasting has been central to business forecasting and recently has received a lot of attention in research. Moving beyond classical top-down and bottom-up alternatives, hierarchical forecasting is nowadays seen as a reconciliation problem. The objective is to adjust independent (base) forecasts by combining them, subject to additivity constraints, to obtain the hierarchical (coherent) forecasts. Although there are multiple approaches to do this, the widely used MinT methodology provides such a solution and has been shown to provide good results. Recently, a geometric interpretation of the methodology provided additional insights into its properties and operation. We rely on this geometric view to investigate alternative solutions to the hierarchical forecasting problem. First, we show that we can provide a general and efficient geometric formulation of the alternative MinT-family reconciliation heuristics, for low dimensional problems, that moves away from the regression formulation of the problem. Then we proceed to propose an exact and an approximate reformulation that both overcome the dimensionality limitation and provide competitive solutions to MinT.

#### Notes:

Showed same geometric explanation as first talk by George Athanasopolous. Showed that MinT is same as choosing the right projection angle. But this is infeasible to do directly?

Can plot these rotations in a rotation plot.

But using the angle plot is useful for diagnosing estimation errors for W. 

### Don't evaluate coherent hierarchical forecasts using the MAPE!
#### Stephan Kolassa
#### Abstract:

On the one hand, we want hierarchical forecasts to be coherent (i.e., sum-consistent: the forecast of a sum total should be the total of the component forecasts). On the other hand, we want low errors, such as MAPEs. On the third hand, optimal point forecasts depend on the error measure. Assessing coherent hierarchical forecasts on MSEs makes sense, because MSEs are minimized by expectation forecasts, and the expectation is additive. However, the MAPE-minimizing functional is not additive, so if we want to minimize the MAPE, it makes no sense to aim for coherent forecasts - and vice versa. This is analogous to the fact that (hopefully) nobody will expect hierarchical quantile forecasts to be coherent. We investigate this effect and call for an increased emphasis on distributional aspects in hierarchical forecasting.

#### Notes:

The contentions
- The only KPI (measure?) that makes sense for heirarchical point forecasting is the RMSE
- We should pay more attention to multivariate predictive densities

Point forecasts depend on error measure. Example with gamma.

Example: quantile forecasts should not be coherant (he means, shouldn't sum up). 

Example: if want median forecast, then we don't want this either. So shouldn't evaluate with MAE. 

Example: similar but for MAPE

Say we should think about joint predictive densities.

Very basic level talk.


### Optimal Top-Down Hierarchical Forecast Reconciliation
#### Robert Davies
#### Abstract:

This paper develops a new method to perform top-down hierarchical forecast reconciliation that is designed to minimize the overall amount of revision to the original, unreconciled forecasts. This would be useful in situations where one needed to, or had a strong reason to, use a top-down hierarchical forecast reconciliation, but wanted to preserve the original, unreconciled forecasts as much as possible. Such situations are common in many business settings where there are often strong priors on the top-level growth of the business or different business metrics. If we define the difference between the original, unreconciled forecasts and the final, reconciled forecasts as a reconciliation error, the method developed in this paper works by minimizing a loss functions over the reconciliation errors subject to the constraint that the top-most forecast remains unaltered. The paper builds off the methods developed in Hyndman et al (2011) and elsewhere, but introduces the concept of using a constrained optimization approach to achieve a top-down reconciliation.

Two backtesting exercises are presented where the method in this paper is contrasted with other popularly used top-down reconciliation methods. The first exercise uses an internal dataset on revenue for different business metrics within Amazon Web Services (AWS). The second exercise uses publicly available data on tourism in Australia. In both exercises the optimal top-down method introduced in this paper had the smallest overall revision to the original, unreconciled forecasts. In the first exercise it had an average absolute revision of 2.40% compared with 4.85% for the next best performing top-down method. In the second exercise it had an average absolute revision of 0.40% compared with 0.89% for the next best performing top-down method. In terms of accuracy, the method introduced in this paper had comparable accuracy results with the best performing of the existing top-down methods considered and not much worse than the accuracy of the original, unreconciled forecasts or the (not top-down) optimal method introduced in Hyndman et al (2011). The method developed here had the lowest average MAPE among all top-down methods in the first backtesting exercise with an average MAPE of 17.60% compared with 18.26% for the next best performing top-down method. The original, unreconciled forecasts had an average MAPE of 16.96% and the optimal method from Hyndman et al (2011) had an average MAPE of 17.29%. In the second backtesting exercise the method developed here had an average MAPE of 4.91% compared with 4.85% for the best performing top-down method. The original, unreconciled forecasts had an average MAPE of 4.86% and the optimal method from Hyndman et al (2011) had an average MAPE of 4.83%.

#### Notes:

Didn't show


### GRAVITATIONAL FORECAST RECONCILIATION
#### Carla Freitas Silveira Netto
#### Abstract:
In many situations, companies do not have time-series of sales data, in geographical disaggregation corresponding to their marketing strategy. Even more challenging, if those companies want to enter a new market or to expand their business to new locations, they will probably arrive at a new territory having just a total sales estimation for the new country or city without any sales history on more local levels. This may be a problem because marketing activities are usually executed at a more local, disaggregated level. This total, aggregated, forecast will need to be divided into smaller geographical regions coherently to the total forecast. This is usually done by applying a statistical approach known as forecast reconciliation. These approaches, so far, use actual sales data or at least historical sales proportions on a disaggregated level. The problem is that such data may not always be available. In this paper, we propose a new forecasting reconciliation approach. We forecast the most aggregated level sales using a deep learning technique -- Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) -- adapted to time-series forecasting and then reconcile them to lower levels with an alternative top-down approach that uses a gravitational model inspired in Huff's gravity model (Huff, 1964). Our gravitational forecast reconciliation (GFR) has the advantage of, instead of using historical proportions, it distributes the total sales among more granular levels with a gravitational model built with easily/publicly available and/or low-cost geographical POI data. We test and validate our approach with historical sales proprietary data of two different companies from two different countries, a Brazilian and a Turkish manufacturer of construction materials, both major players in their respective countries and markets. The results show that our proposed reconciliation approach has a similar or better performance than the benchmark approaches at all levels of the geographical aggregation tested. The key theoretical contribution is to demonstrate that gravitational models based on agglomeration theory (Liu, Steenkamp, & Zhang (2018) can be used to predict and not only explain behavior. Our methodological contribution enables companies to distribute and reconcile forecasts to lower levels of aggregation when no actual disaggregated sales data or historical sales proportions are available. For practice, the GFR approach offers several advantages. First, companies can apply it to new territories, points of sales, or marketing channels, distributing the forecasted aggregated sales into smaller granular spatial units. Second, GFR also allows the development of a coherent forecast system based on the location, quantity, and diversity of points of interest (POIs) of a specific region, according to the desired company strategy. Third, it is easy and relatively cheap to implement in any organization, from all sizes and economic sectors, helping them to distribute sales forecasts to new regions or markets where the company plans to direct the market and have no historical sales data, just a total estimation. Finally, GFR can be embedded in production planning decision support systems and help companies to devise their future marketing strategies.

#### Notes:

Situation where you want to do top down reconciliation. They propose a method they called 'gravitational forecast reconciliation'.  Based on the Huff Gravity model. 

This is a model of customers living in population centers, with shopping facilities. Individual based model that includes euclidian distance between population centre and facility (determining proportions for the top down model). Does not require historical data or proportions because you use the model.

Have data on 
- demographics + geography
- economic indicators
- POIs
- something else


Only have forecast for aggregate level + need to disaggregate to geographical areas = can use this method.
