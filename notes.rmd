---
title: "ISF 2020 notes"
author: "Timothy Hyndman"
date: "26/10/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    collapsed: false
    smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Session: Global Modelling for Forecasting

### Transfer learning schemes for global forecasting models using recurrent neural networks
#### Kasun Bandara
#### Abstract: 
Forecasting models that are trained across a set of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN) that require sufficient amounts of time series to estimate their numerous model parameters. Many time series databases however suffer from having only a limited number of time series. The transfer learning (TL) methodology has been proposed to overcome the limitation of training data by enabling knowledge transfer from models trained on a source task with sufficient amount of training data to a target task with few training instances. Although several studies have been conducted to forecast better under these circumstances, how to accurately forecast with GFMs in a situation with less data has not yet been thoroughly studied. To alleviate this problem, in this study, we investigate to what extent the TL methodology is useful, when using RNNs for GFM training. We propose a Residual RNN stacking architecture, supplemented by various TL schemes to overcome the challenges of time series sparsity, when developing GFMs. Using a comprehensive model training plan, we demonstrate the effectiveness of our proposed TL schemes for leveraging the capabilities of GFMs to forecast with a limited amount of time series. Furthermore, in a situation where time series data is not available to build a pre-trained model for knowledge transfer, we use GRATIS, a statistical generative model that artificially generates time series with diverse characteristics. In our evaluation on the kaggle web traffic forecasting challenge dataset, our proposed TL schemes are able to improve the baseline accuracy of GFM models, and outperform state-of-the-art univariate forecasting methods. The results also indicate that our approach can be operated to improve the forecasting accuracy, even in the presence of a single time series.

#### Notes:

- Global model: learns parameters from all series
- Don't perform so well if you don't have much data. How do you deal with this?

1. Augment the data by generating extra stuff
- GRATIS
- DBA
- STL + MBB

Gratis generates time series with diverse characterstics, the STL+MBB and DBA generate similar time series.

2. Can also use transfer knowledge (as well as? or as a different option?)

He used RNN with residual connections.

Stopped being able to follow from here...

Tested his method out on a bunch of data and compared to prophet, autoarima, and smooth::es. Results varied based on the data, but generally performed better than the benchmarks. Took much more computation than the statistical methods. MBB worked better than DBA for AusEnergy dataset - probably because it was smaller? Both DBA and MBB did better than GRATIS. 

Summary: Can still use RNNs without big data if you can augment your data (and maybe do some transfer learning?). The best methods are DBA and STL+MBB. Still depends on the data as to whether you can outperform statistical methods.


### Automatic feature-based forecast model averaging
#### Alexey Chernikov
#### Abstract:
Time series features are widely used in Big Data environments due to their capabilities in noise reduction, dimensionality reduction, and capturing relevant information of an object. Their use in time series classification achieves state-of-the-art accuracy and has become a de-facto standard. In forecasting, recent studies also demonstrate that time series features are able to contribute to achieve good results, e.g., in combination approaches with meta-learner based models. For example, the second place in the M4 Forecasting Competition was won by Feature-based Forecast Model Averaging (FFORMA), a model that exploits cross-series information in a meta-learner environment. In this model, the cross-series information is obtained using 42 statistical features that then serve as an input into the meta-learner. Despite the good result achieved with hand-crafted features, the approach has the potential limitations that hand-crafted features require domain knowledge and significant data-engineering work, which oftentimes is not feasible and may make the features not flexible enough to adapt to new datasets.

In this study we propose a novel approach of a meta-learner with time series features. We replace the hand-crafted features with automatically generated features of time series in FFORMA. In particular, We present a deep-learning method to extract time series features specifically for forecasting using a convolutional neural network. We also demonstrate the impact our features have on the prediction capabilities of meta-learner based models like FFORMA, and compare the results with a host of benchmarking methods, and in particular to methods based on conventional statistical features. We are able to obtain promising results in various application areas, in particular on the M4 dataset, even with significantly smaller amounts of features than the comparison methods.

#### Notes:
Feature based forecasting eg
- FFORMA
- FFORMS 
- etc

FFORMA uses TS features as a core component for combining models. Came second in M4 competition.
Uses handcrafted features (tsfeatures, catch22, tsfresh). Disadvantages:
- computationally intensive
- low discriminative power
- require domain knowledge
- feature engineering

Their research was into automating this stuff. Come up with an autoencoder. There was some problem with this, I think to do with not encoding based on the whole series, just using parts of it.

Static features: describe whole time series
Dynamic: describe time series in moment of time (look at a window)

Wanted to extract static features, but autoencoder only gives you dynamic features.

Use CNN to take dynamic features and turn into static features. Got 16 features which they then tested out on M4 data. Clustering on these features results in clusters with similar looking time series. 

Using these 'deep features' results in better performance that fforma with tsfeatures (and other benchmarks) in most benchmarks, others were close. This is important because FFORMA is based on M4 data to begin with (so automatic features does better than expert selection).

Also

- Used fewer features (42 vs 16)
- Not sensiitve to length of series
 

### Local model-agnostic interpretability in global time series forecasting
#### Dilini Rajapaksha
#### Abstract:
Forecasting models that are trained across a set of multiple related time series, known as Global Forecasting Models (GFM), have shown promising results in forecasting competitions and real-world applications while outperforming many typical univariate forecasting approaches. One aspect of the popularity of forecasting models such as ETS and ARIMA is their relative simplicity and interpretability which enables the trust of the users towards the forecast. The main drawback of more complex global models, such as globally trained neural networks (NN), is their lack of interpretability, especially localized to a particular time series. This reduces the trust and confidence of the stakeholders when making decisions based on the forecasts without being able to understand the predictions. To mitigate this problem, in this work, we propose a novel local model-agnostic interpretability approach to explain the forecasts given from forecasting models that are built using global modelling techniques such as NNs. The proposed framework is able to explain the forecast given from a global model for a particular time-series while considering the forecasting model as a black-box model, in a model-agnostic way. To achieve this we train simpler univariate models that are considered interpretable (e.g., ETS) on the time series which needs to be explained while minimizing the forecasting difference between the global black-box model and the statistical model. As the results of the proposed approach, the forecast produced by the black-box model can therewith be explained through components such as relevant lags, trend, seasonality, and potentially the influence of external factors, thus enabling the user to better understand the forecast.

#### Notes:
Generally simple models are more interpretable and less accurate, and complex models are more accuracte but less interpretable. So there is a trade off. To overcome this trade off, find an explanation of complex models. Enter "Local Interpretability". Common in Machien learning, uses local neighbourhood to explain particular predictions. 

In time-series forecasting we have local and global forecasting. Want to use local interpretability for global model. 

Global models have higher in-sample error and generalise better (i.e. doesn't over fit). 

Not sure exactly how the local explainers were generated.. something to do with fitting simple models (eg es, prophet etc) on the 'fit of the global model'?

Lost the flow of this talk after this.

### Addressing data heterogeneity in time series forecasting using global ensemble models
#### Rakshitha Godahewa
#### Abstract:
The performance of Neural Networks (NN) was poor compared to the traditional univariate forecasting models in many forecasting competitions in the past. But recently, global forecasting models have demonstrated that they have the potential to outperform traditional univariate forecasting models such as Exponential Smoothing State Space Model (ETS) and Autoregressive Integrated Moving Average (ARIMA) after contributing to the winning solution of the M4 forecasting competition. However, if a time series database contains heterogeneous series, the accuracy of the global forecasting models may degrade. This can happen especially through underfitting the data.
To bridge this gap, in this study, we focus on improving the accuracy of global forecasting models by addressing data heterogeneity using ensembling approaches. We present a series of ensemble forecasting models based on linear regression, Feedforward Neural Networks (FNN) and Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) cells. These models aggregate the predictions provided by a series of sub-models trained over different subgroups of time series within a dataset. The implemented ensemble models include clustering techniques that incorporate feature clustering, distance-based clustering and random clustering and the ensembling technique used for the winning solution of the M4 forecasting competition; the ensemble of specialists. We further combine the forecasts of our global models and traditional univariate forecasting models to improve the forecasting accuracy. Over six publicly available datasets, we show that ensembling a series of models trained over different parts of a particular dataset can improve the forecasting accuracy compared to non-ensemble models based on four evaluation metrics.

#### Notes:
- GFMs may not be so good is the time series are heterogeneous (diverse characteristics)
- Can train multiple GFMs for groups of similar time series (eg use clustering on series)

Can also use ensembling - aggregating predictions provided by a bunch of models.

2 main types - sequential (boosting), or parallel (bagging)

Also aggregating (averaging methods)

Considered three base methods (FFNN, RNN, and pooled regression). 
clustering methods (features, distance-based (DTW))
ensemble methods too

Conclusion: Clustered ensemble models work well for heterogeneous datasets


### Global models for time series forecasting: a simulation study
#### Hansika Hewamalage
#### Abstract:
Many state-of-the-art statistical techniques used for forecasting since the early days are univariate forecasting techniques which build a model per every series in isolation. However, with the recent advances in the infrastructure for storage, companies have collected enormous amounts of data over the years. Consequently, in the current context of Big Data, the nature of the forecasting problem has changed from predicting isolated series to predicting many (possibly related) series. The availability of large sets of time series opens up the opportunity to develop global models which simultaneously learn from many time series as opposed to traditional univariate techniques. Specifically in this respect, Recurrent Neural Networks (RNN) have demonstrated promising performance, e.g. in the M4 forecasting competition. Nevertheless, it still remains unclear under which circumstances global forecasting models can outperform the univariate benchmarks, especially along the dimensions of homogeneity/heterogeneity of series, complexity of patterns in series, complexity of models, and the lengths/number of series.

Our study investigates these issues, by simulating a number of datasets that have controllable characteristics. We perform experiments involving a number of global forecasting models including RNNs, pooled regression models and Feed-Forward Neural Networks within these simulated settings and compare their performance against standard statistical univariate benchmarks. In this manner, we seek to comprehend the behaviour of global forecasting models when encountered with datasets of specific characteristics. In our work, we introduce complexity into the time series in terms of the number of parameters of the Data Generating Process (DGP) as well as its non-linearity. The heterogeneity is introduced by mixing time series generated from several DGPs into a single dataset. Unlike the real-world datasets, the simulated environments render it possible to have complete control over the underlying datasets. Our initial experiments demonstrate that developing a global model across many short series helps better than developing many models on every individual short series on a homogeneous set of series. Also, as is to be expected, linear pooled regression models are competitive in fitting the series coming from a linear DGP. When it comes to more complex DGPs in terms of non-linearity, the complex modelling capabilities of the RNNs help further. Similarly, with respect to heterogeneous series with multiple simple DGPs as well, it is evident that RNNs can show superior performance as global models with their complex functionality. Thus, we are able to characterise trade-offs between the heterogeneity, complexity of the series and the forecasting performance of global models.

#### Notes:

Global models work well on 'related' time series. But what does 'related' mean in this context?

They simulate a number of time series, then run experiments on how related the time series have to be for global models to work well.

Generate data with AR models, seasonal AR models, Mackey-Glass Equation, SETAR model, chaotic logistic map, 
Forecast with RNN (lstm cells with peephole connections), FFNN, LGBM, pooled regression, univariate AR models

Did a bit of data preprocessing
- Normalisation (helpful esepcially for global models)
- Log transforms
- Moving window creation
- Window normalisation

Bunch of experimental scenarios, varying aspects of the series, and the models.

Different models do well in different scenarios - RNNs seem to do okay with Heterogenous series. LGBM good in some scenarios. PR good in others. AR and FFNN never the best. 

- PR good for simple homogeneous patterns. 
- More complex needs LGBMs and RNNs
- For heterogeneous series, use complex models again, even if individual series are simple. 
- RNNs, LGBMs competitive in variety of situations
- AR, PR make assumptions which might not hold
- LGBMs very good computationally, especially compared to RNNs
- Local models get more complex with more series, and computation time increases with nubmer of series too

### Why does joint forecasting of multiple time series work so well?
#### Pablo Montero-Manso
#### Abstract:
Organizations usually need to forecast multiple time series coming in groups, such as the demands for all products offered at one store, guest arrivals to all accommodation facilities at one city, etc. Local forecasting methods model each series individually, they fit a model to each product category or each accommodation facility. Global methods fit only one model for the whole group, pooling together the data for all time series. We can expect global methods to work better when the time series in the group are of similar nature, and local would be best when the group is heterogeneous. 

However, in the last few years global methods have been outperforming local methods even in some highly heterogeneous datasets. There is a lack of understanding of this phenomenon and the underlying principles that guide it. In particular, practitioners interested in the adoption of global methods face difficulties such as: When should we use global methods? How to group series? What constitutes similar series? What types of models are suitable for global methods? What sort of preprocessing do we need? What are the tradeoffs between local and global methods?

We will present theoretical and empirical results comparing global and local methods, focusing on two main ideas:

1) Global methods can be as accurate as local for any set of time series, including very heterogeneous sets. The strong assumption of global methods to fit a single model for all series in the set is not restrictive for forecasting.

2) The complexity of local methods grows with the size of the set. For example, even a simple local method such as exponential smoothing can be more complex than a global deep neural network when considering a large dataset.

We will show how these results can be applied in practice for designing new forecasting algorithms that improve over the state-of-the-art local methods.

#### Notes:
Local methods: statistical but also NN, trees etc when fit on single series
Global methods: 

We compare generalization gaps of local and global methods. The generalization gap increases with the number of parameters of the model, the more parameters, the worse it generalizes. (eg see occam's razor, akaike information criteria). 

IMagine we have dataset of 100 time series. Using local method like seasonal arima, each model needs about 2-5 parameters, plus differences. So around ~500 parameters for the whole dataset

Using a global method, if you can get to the same training error with fewer than 500 parameters, you will have better generalisation.

Broadly speaking, model complexity of local methods grows with number of series, but complexity of global methods is fixed. 

BUUTTTT
- multi-task learning not been that successful in general (really?)
- useless for really complex models (more params than samples (what does this mean))
- does not explain why we can fit really simple global models

##### Approximation capacity of global methods

why can we fit different time series with the same model.

Given a local method, there exists a global method that produces texactly the same predictions

- Does not mean that they are easy to find
- think about 'true' data generating processes for each time series
- set of time series does not matter
- it does not happen in general outside of time series

Example of how you can use AR model to exactly generate any degree 2 polynomial. So need just 1 global model with 3 parateters, vs each local model needing 3 parameters if you model each as degree 2 polynomial.

Show a similar thing with the M4 dataset

- Global methods are specific type of multi-task learners
- Can approximate as well as local methods
- IN practice can find very simple global models that fit large datasets
- in the absence of strong prior knowledge that favors one particular local method, global methods will be able to exploit the data much better.

# Session: Probabilistic Forecasting
### Optimal probabilistic forecasts: When do they work?
#### Ruben Loaiza-Maya
#### Abstract:
Proper scoring rules are used to assess the out-of-sample accuracy of probabilistic forecasts, with different scoring rules rewarding distinct aspects of forecast performance. Herein, we re-investigate the practice of using proper scoring rules to produce probabilistic forecasts that are `optimal' according to a given score, and assess when their out-of-sample accuracy is superior to alternative forecasts, according to that score. Particular attention is paid to relative predictive performance under misspecification of the predictive model. Using numerical illustrations, we document several novel findings within this paradigm that highlight the important interplay between the true data generating process, the assumed predictive model and the scoring rule. Only if a predictive model is sufficiently compatible with the true process to allow a particular score criterion to reward what it is designed to reward, will this approach to optimization reap benefits. Subject to this compatibility however, the superiority of the optimal forecast will be greater, the greater is the degree of misspecification. We explore these issues using a range of different scenarios, and using both artificially simulated and empirical data.

#### Notes:
Missed, due to previous session going over time

### Prediction of harvest date of shrimp in an aquaculture system
#### Ismael Sanchez
#### Abstract:
Shrimp is among the most valuable traded marine product in the world. Accurate prediction of both growing curve and optimal harvest date of shrimp in farms is, therefore, very important. Quality engineers can use it to detect a low growing of the shrimps. Managers can use it to negotiate prices in advance and organize the harvesting logistics. In this work, we build a forecasting system to provide the predictive density of the weight of the shrimp for the future weeks. Alternative approaches are compared, such us the use of growth models and nonparametric techniques. We apply the methodology to a large scale fish farm in northern Peru.
#### Notes:
Predicting shrimp growth in a pond - use similar trajectories in data to help build predictive models. Use predictive errors from similar trajectories or something like that I think

### Identification of factors and forecasting the possibility of human injuries due to an accident in colombo-katunayake expressway, Sri Lanka
#### Kushan Munasinghe
#### Abstract: 
Accidents are one of the main social problems in the world as well as developing countries like Sri Lanka, which cause damage or injuries unintentionally and unexpectedly. Sri Lanka's expressway system was launched in 2011 and currently has three major expressways: Southern Expressway, Colombo-Katunayake Expressway, and Outer-Circle Expressway. After the construction of expressways, many people opted for expressways on the basis of time, traffic, ease of driving, etc., rather than normal roads. The number of accidents on expressways has been on the rise in recent years. Nowadays, the rate of accident occurrence in Colombo-Katunayake Expressway is high compared to the other two Expressways and there was no previous research has been done in Sri Lanka regarding accidents in Colombo-Katunayake expressway. The aim of this study is to identify the factors that contribute to the accidents on the Colombo-Katunayake expressway and to develop appropriate regression and data mining models to forecast the possibility of human injuries due to an accident with a higher level of accuracy.

In this study, 704 total accident cases of Colombo-Katunayake expressway were considered during the period from 2013 to 2019. Pearson Chi-square, Logistic regression, Point biserial correlation and Kruskal–Wallis H tests were used to identify the association between the binary response variable (possibility of human injuries) and other influential variables. Finally, from selected predictor variables, seven variables: time category, driver’s age category, vehicle type, reason for accident, number of vehicles involved, cause for accident and rainfall were identified as influencing variables to the possibility of human injuries under 5% level of significance. Initially, the Binary Logistic Regression (BLR) model was performed to forecast the possibility of human injuries. Further, Classification tree (CART), Naïve Bayes classification algorithm, and Probabilistic neural network (PNN) were used in the study as data mining techniques to forecast the possibility of human injuries for the same datasets which were used to build and test the BLR model. Random Under-Sampling technique was used to overcome the class imbalance problem that persists in the data set considered in the study. Finally, Best performed models were found from each approach and the forecasting ability of these models was compared to come up with the best model to predict the possibility of human injuries.

The BLR model predicts the possibility of human injuries due to accidents with an accuracy of 70.7% and among data mining models, the PNN exhibits a higher overall prediction accuracy than classification tree and Naïve Bayes. The best PNN outperforms the best BLR model and finally, this model can be proposed as a better model to forecast the possibility of human injuries in Colombo-Katunayake expressway. The best performed PNN model contains two hidden layers, with 176 neurons in the pattern layer and two neurons in the output layer with 0.1 spread value. This model has the ability to forecast unseen data with 75.7% accuracy, 78.7% precision and 92.3% recall. The final model developed by this study can be used to implement safety improvements against traffic accidents in expressways of Sri Lanka.

#### Notes:
Didn't listen.


# Session: Hierarchical Forecasting I

### Forecast reconciliation: A geometric view with new insights on bias correction
#### George Athanasopoulos
#### Abstract:
A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular, a general framework is established nesting many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First, reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric. Second, the MinT method minimises expected loss for this same class of loss functions. Third, the geometric interpretation provides a new proof that forecast reconciliation using projections results in unbiased forecasts provided the initial base forecasts are also unbiased. Approaches for dealing with biased base forecasts are proposed. An extensive empirical study on Australian tourism flows demonstrates the theoretical results of the paper and shows that bias correction prior to reconciliation outperforms alternatives that only bias-correct or only reconcile forecasts.

#### Notes:

Generalised perspectives on forecast reconciliation. 

- Hierarchical time series defined by linear constraints (not just sums)
- Reconciliation via projection improves forecast accuracy w.r.t a certain class of loss functions
- 
- 

Can define time series as series which lie in coherent subspace. For example, 
$Y_T = Y_A + Y_B$ forms a plane with basis vectors (1, 1, 0) and (???). When we reconcile a forecast, we are mapping incoherent forecasts to the coherent subspace. Do this by pre-multiplying base forecasts by a nxn matrix that has $s$ (the subspace) as its image. 

So the most obvious is to use a Euclidean orthogonal projection. This guarentees that
- Base and reconciled forecasts are as close as possible
- Reconciled forecasts are closer to actuals than base forecasts

However, this measure is the sum of squared errors across ALL series. Consider differences in scales and predictability. May want to suggest different weights.

Using weights in the loss function you end up with oblique projections. You can then show that property two above still holds using a norm based on the weights.

MinT is a unique oblique projection that minimises the trace of your covariance matrix. There was a neat geometric explanation of how this minimises forecast errors of the reconciled forecasts.

Empirical results: Looked at australian tourism data. Use auto.arima() to generate 1 step ahead forecasts. Use 4 different projections, based on different loss functions. Each projection should be best for the different loss functions. The projection for the loss function results in errors being strictly better than base functions. MinT does best on average for everything though.

So can either
- Choose to guarantee that reconciled forecasts improve upon base forecasts
- find the reconciliation method that is best on average

Fantastic talk!!!


### Probabilistic Forecasts in Hierarchical Time Series
#### Anastasios Panagiotelis

#### Abstract:
We develop a framework for prediction or forecasting of multivariate data that follow some known linear constraints, such as the example where some variables are aggregates of others. For point prediction, an increasingly popular technique is reconciliation, whereby predictions or forecasts are made for all series (so called ‘base’ forecasts) and subsequently adjusted to ensure coherence with the constraints. This paper extends reconciliation from the setting of point prediction to probabilistic prediction. A novel definition of reconciliation is developed and used to construct densities and draw samples from a reconciled probabilistic prediction. In the elliptical case, it is proven that the true predictive distribution can be recovered from reconciliation even when the location and scale matrix of the base prediction are chosen arbitrarily. To find reconciliation weights, an objective function based on scoring rules is optimised. The energy and variogram scores are chosen since the log score is improper in the context of comparing unreconciled to reconciled forecasts, a result also proved in this paper. To account for the stochastic nature of the energy score, optimisation is achieved using stochastic gradient descent. The proposed algorithm is applied in a number of simulated scenarios and to the empirical example of forecasting electricity generation from different sources (e.g, renewable v non-renewable, coal, gas, wind solar etc.).

#### Notes:

https://anastasiospanagiotelis.netlify.app/talks/probreco/probreco#1

Can do the same definition for coherence, but on probabilities, instead of points. End up using some measure theory to talk about images, instead of pre-multiplying by a matrix, and pre-images instead of inversions.

Take the probability from the space (A), and assign that probability to the image on the subspace.

They proved:
- Reconciling a sample from base distribution, gives sample from reconciled distribution
- If base is elliptical, then reconciled is elliptical (eg normal)
- For any mu and sigma, there exists a d and G (a reconciliation) that recovers the true prediction distribution (but it's not feasible to find these! and won't always be a projection)

So need to find a way to find d and G (and not necessarily use a projection). 

We use a scoring function and optimise with respect to this scoring. Example
- log score. But doesn't satisfy some desired properties
- energy score
- variogram score

Got a little lost here... you use scoring function to find d and G...


### A geometry inspired hierarchical forecasting methodology
#### Nikos Kourentzes
#### Abstract:

Hierarchical forecasting has been central to business forecasting and recently has received a lot of attention in research. Moving beyond classical top-down and bottom-up alternatives, hierarchical forecasting is nowadays seen as a reconciliation problem. The objective is to adjust independent (base) forecasts by combining them, subject to additivity constraints, to obtain the hierarchical (coherent) forecasts. Although there are multiple approaches to do this, the widely used MinT methodology provides such a solution and has been shown to provide good results. Recently, a geometric interpretation of the methodology provided additional insights into its properties and operation. We rely on this geometric view to investigate alternative solutions to the hierarchical forecasting problem. First, we show that we can provide a general and efficient geometric formulation of the alternative MinT-family reconciliation heuristics, for low dimensional problems, that moves away from the regression formulation of the problem. Then we proceed to propose an exact and an approximate reformulation that both overcome the dimensionality limitation and provide competitive solutions to MinT.

#### Notes:

Showed same geometric explanation as first talk by George Athanasopolous. Showed that MinT is same as choosing the right projection angle. But this is infeasible to do directly?

Can plot these rotations in a rotation plot.

But using the angle plot is useful for diagnosing estimation errors for W. 

### Don't evaluate coherent hierarchical forecasts using the MAPE!
#### Stephan Kolassa
#### Abstract:

On the one hand, we want hierarchical forecasts to be coherent (i.e., sum-consistent: the forecast of a sum total should be the total of the component forecasts). On the other hand, we want low errors, such as MAPEs. On the third hand, optimal point forecasts depend on the error measure. Assessing coherent hierarchical forecasts on MSEs makes sense, because MSEs are minimized by expectation forecasts, and the expectation is additive. However, the MAPE-minimizing functional is not additive, so if we want to minimize the MAPE, it makes no sense to aim for coherent forecasts - and vice versa. This is analogous to the fact that (hopefully) nobody will expect hierarchical quantile forecasts to be coherent. We investigate this effect and call for an increased emphasis on distributional aspects in hierarchical forecasting.

#### Notes:

The contentions
- The only KPI (measure?) that makes sense for heirarchical point forecasting is the RMSE
- We should pay more attention to multivariate predictive densities

Point forecasts depend on error measure. Example with gamma.

Example: quantile forecasts should not be coherant (he means, shouldn't sum up). 

Example: if want median forecast, then we don't want this either. So shouldn't evaluate with MAE. 

Example: similar but for MAPE

Say we should think about joint predictive densities.

Very basic level talk.


### Optimal Top-Down Hierarchical Forecast Reconciliation
#### Robert Davies
#### Abstract:

This paper develops a new method to perform top-down hierarchical forecast reconciliation that is designed to minimize the overall amount of revision to the original, unreconciled forecasts. This would be useful in situations where one needed to, or had a strong reason to, use a top-down hierarchical forecast reconciliation, but wanted to preserve the original, unreconciled forecasts as much as possible. Such situations are common in many business settings where there are often strong priors on the top-level growth of the business or different business metrics. If we define the difference between the original, unreconciled forecasts and the final, reconciled forecasts as a reconciliation error, the method developed in this paper works by minimizing a loss functions over the reconciliation errors subject to the constraint that the top-most forecast remains unaltered. The paper builds off the methods developed in Hyndman et al (2011) and elsewhere, but introduces the concept of using a constrained optimization approach to achieve a top-down reconciliation.

Two backtesting exercises are presented where the method in this paper is contrasted with other popularly used top-down reconciliation methods. The first exercise uses an internal dataset on revenue for different business metrics within Amazon Web Services (AWS). The second exercise uses publicly available data on tourism in Australia. In both exercises the optimal top-down method introduced in this paper had the smallest overall revision to the original, unreconciled forecasts. In the first exercise it had an average absolute revision of 2.40% compared with 4.85% for the next best performing top-down method. In the second exercise it had an average absolute revision of 0.40% compared with 0.89% for the next best performing top-down method. In terms of accuracy, the method introduced in this paper had comparable accuracy results with the best performing of the existing top-down methods considered and not much worse than the accuracy of the original, unreconciled forecasts or the (not top-down) optimal method introduced in Hyndman et al (2011). The method developed here had the lowest average MAPE among all top-down methods in the first backtesting exercise with an average MAPE of 17.60% compared with 18.26% for the next best performing top-down method. The original, unreconciled forecasts had an average MAPE of 16.96% and the optimal method from Hyndman et al (2011) had an average MAPE of 17.29%. In the second backtesting exercise the method developed here had an average MAPE of 4.91% compared with 4.85% for the best performing top-down method. The original, unreconciled forecasts had an average MAPE of 4.86% and the optimal method from Hyndman et al (2011) had an average MAPE of 4.83%.

#### Notes:

Didn't show


### GRAVITATIONAL FORECAST RECONCILIATION
#### Carla Freitas Silveira Netto
#### Abstract:
In many situations, companies do not have time-series of sales data, in geographical disaggregation corresponding to their marketing strategy. Even more challenging, if those companies want to enter a new market or to expand their business to new locations, they will probably arrive at a new territory having just a total sales estimation for the new country or city without any sales history on more local levels. This may be a problem because marketing activities are usually executed at a more local, disaggregated level. This total, aggregated, forecast will need to be divided into smaller geographical regions coherently to the total forecast. This is usually done by applying a statistical approach known as forecast reconciliation. These approaches, so far, use actual sales data or at least historical sales proportions on a disaggregated level. The problem is that such data may not always be available. In this paper, we propose a new forecasting reconciliation approach. We forecast the most aggregated level sales using a deep learning technique -- Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) -- adapted to time-series forecasting and then reconcile them to lower levels with an alternative top-down approach that uses a gravitational model inspired in Huff's gravity model (Huff, 1964). Our gravitational forecast reconciliation (GFR) has the advantage of, instead of using historical proportions, it distributes the total sales among more granular levels with a gravitational model built with easily/publicly available and/or low-cost geographical POI data. We test and validate our approach with historical sales proprietary data of two different companies from two different countries, a Brazilian and a Turkish manufacturer of construction materials, both major players in their respective countries and markets. The results show that our proposed reconciliation approach has a similar or better performance than the benchmark approaches at all levels of the geographical aggregation tested. The key theoretical contribution is to demonstrate that gravitational models based on agglomeration theory (Liu, Steenkamp, & Zhang (2018) can be used to predict and not only explain behavior. Our methodological contribution enables companies to distribute and reconcile forecasts to lower levels of aggregation when no actual disaggregated sales data or historical sales proportions are available. For practice, the GFR approach offers several advantages. First, companies can apply it to new territories, points of sales, or marketing channels, distributing the forecasted aggregated sales into smaller granular spatial units. Second, GFR also allows the development of a coherent forecast system based on the location, quantity, and diversity of points of interest (POIs) of a specific region, according to the desired company strategy. Third, it is easy and relatively cheap to implement in any organization, from all sizes and economic sectors, helping them to distribute sales forecasts to new regions or markets where the company plans to direct the market and have no historical sales data, just a total estimation. Finally, GFR can be embedded in production planning decision support systems and help companies to devise their future marketing strategies.

#### Notes:

Situation where you want to do top down reconciliation. They propose a method they called 'gravitational forecast reconciliation'.  Based on the Huff Gravity model. 

This is a model of customers living in population centers, with shopping facilities. Individual based model that includes euclidian distance between population centre and facility (determining proportions for the top down model). Does not require historical data or proportions because you use the model.

Have data on 
- demographics + geography
- economic indicators
- POIs
- something else


Only have forecast for aggregate level + need to disaggregate to geographical areas = can use this method.

# Keynote: Rob Hyndman

### Ten years of forecast reconciliation

#### Notes:

##### Hierarchical forecasting 20 years ago
In 1990s there were really three choices
- Top down
- Middle out
- Bottom up

Around 2001 had the idea to reconcile all forecasts using least squares optimization. Then 2004 had PhD student to start working on it with postdoc (George). Presentation at ISF 2006, pre-print paper in 2007. Application to Australian tourism data in 2009. First version of hts package on CRAN 2010. 2011 until paper finally appeared.

Subject has really taken off in the last 10 years.

##### Point forecast reconciliation
Can have 
- Hierarchical series
- Grouped series

eg tourism by state and purpose of travel (order doesn't matter so grouped)

Can always write as

$$y_t = S b_t$$

$y_t$ is vector of all series
$b_t$ vector of most disaggregated series
$S$ summing matrix containing linear constraints

Reconciliation is mapping from $\mathbb{R}^n$ to a coherent subspace. Almost all of the work is about linear reconciliation (the mapping is linear).

$$\tilde{y} = S G \hat{y}$$

In this scenario you can show that reconciled forecasts are unbiased, and get an expression for the variance.

Minimum trace (MinT) reconciliation - trace of variances is minimized for a know expression. 

Hard to calculate this expression. There are a bunch of estimates for this (both for MinT and for OLS and WLS). 

##### Example: Australian tourism
All methods give better accuracy than base forecasts. Can't use mint_cov for lots of series! Improvements greater at lower levels

##### Probabilistic forecast reconciliation
What does it mean for a probabilistic forecast to be reconciled? 
- The reconciled multivariate density must lie on the coherent subspace.

Two results:

- If the incoherent base forecasts are normal, then the reconciled density is normal (with given mean and variance).
- Can reconcile sample paths to get a reconciled distribution

Difficult to evaluate probabilistic forecasts in a way that works - needs to use a proper scoring rule (eg pinball loss but integrated over all possibilities). Eg shouldn't use log score because it's not proper

Proper = minimized when you have the 'truth'.

Energy score is proper (but you have to use FULL hierarchy).

Have to optimize $G$ to optimize Energy score now, instead of using MinT. 

##### Example
Energy forecasting. Interested in probabilistic because of extreme tail events. Use simple feed forward NN to forecast. Residuals are non-gaussian. Strong correlations between some residuals.

Computed mean energy score using 4 different assumptions on residuals (gaussian/dependence). Used a bunch of different methods. SKipped over the results.

##### Extensions
What else have people done in this space?

- $S$ matrix can have any values, not just 0 and 1
- Linear regression + reconciliation can be done in one go.
- Non negative reconciliation
- Temporal reconciliation + cross-temporal reconciliation
- Bayesian forecast reconciliation
- ML and regularization


# Keynote: Best practices for forecasting at scale
#### Ginger Holt (Manager, Infrastructure Data Science Facebook) and Italo Lima (Data Scientist, Infrastructure Data Science Facebook). 

#### Notes:

##### Organizational Challenges
Infrastructure Data Science (IDS)

- Accuracy isn't always what they want
- Series have different levels of volatility
- Recruiting. They recruit generalists, need technical skills + cross-functional ability. Non-ML emphasis.

Resource Allocation
- Data storage (need to store forecasts + actuals all the time so can evaluate accuracy later)
- Tooling - automation, fast computation, parallelization, customization, visualization.

- Emphasis on documentation
- people have major and minor areas of focus

A lot of fluff talked about here without much of substance.

##### Practical Challenges

- Onboarding forecasts
- Data QC
- Model selection
- Forecast publication/presentation
- Automatic monitoring

##### Technical Challenges

- Holidays and special events: peak prediction. Hard to forecast directly. Use two methods, overall prediction, and then separate prediction for peaks.
- Prediction intervals / uncertainty intervals: periodically refresh the forecast and keep a close eye on error monitoring (lots of error metrics). 
- Hierarchical reconcilation: Multiple FB products, geographical changes (lower levels are noisy)
- Temporal reconciliation
- Model comparison: use many related forecasts for comparison. Depends a lot on which metric they are optimising for.
- Hyperparameter tuning: manually set parameteers (i.e. not trained). Prophet has 15 hyper parameters for trend, seasonality, simulation

To find them Create a good API
- Set evaluation function
- Create a search agent
- Either grid search, or some version of random search

"Self-supervised learning for fast and scalable time-series hypterparameter tuning" - Xiaodong Jiang, Peiyi Zhang, et al.
Model selection with self-supervised learning

Tooling: 

##### Solutions
##### Q&A


# Software and Support Systems I

### Forecasting website
#### Andrey Vasnev
#### Abstract:
Forecastability builds confidence and certainty in the economy. Both individuals' and organizations’ decision-making processes become more efficient, given a reliable and robust forecast of the economy. Inaccurate forecasts, whether they underestimate or overestimate, incur additional costs. Forecasts are therefore crucial for all economic and business activities. Forecasting accuracy, especially over longer horizons, is crucial for policymakers, institutions, and individuals.

The purpose of this project is to produce a high-quality website that makes business forecasting accessible to the wider community. The goal is to make the website a default ‘go-to’ place for everyone who needs to predict common publicly available time series.

The proposed publicly available forecasting website aims to implement both classical forecasting models as well as the novel models. The website will visualize and summarize the forecasting results in an easy-to-understand manner. Regular macroeconomic and financial forecasts will be produced on a quarterly and daily basis respectively.

The Time Series and Forecasting group at the University of Sydney Business School has built a website, which is already operational and available at <business-forecast-lab.com>. We implemented daily automatically updated forecasts for major financial time series, including exchange rates, stock indices, and volatilities across different countries and markets. The models were implemented with Python, which is running on a server automatically every day.

#### Notes:
The current state:
- lots of data available online
- lots of forecasting methods freely available
- performance of these methods is reliable
- but it is difficult to get a current forecast of commonly used series

The idea:
- Create an open-source forecasting website

See https://business-forecast-lab.com/

Uses python docker container with rpy2 for R packages. Updates data and forecasts once a day. Webserver uses gunicorn and plot.ly dash (so no javascript I think).

Community sourced so open to suggestions

### Probabilistic cross-temporal hierarchies in fable
#### Mitchel O'Hara-Wild
#### Abstract:
The fable framework facilitates forecasting large collections of time series within a tidy data analysis workflow. The framework provides a carefully designed modelling interface which allows complex modelling tasks to be performed with minimal and readable code.

Recent extensions allow fable to better handle time series that are observed frequently at various levels of a hierarchy. It is now possible use fable for constructing and reconciling temporal hierarchies, which is a technique well known to improve forecasting accuracy. Temporal reconciliation can also be used on an existing hierarchy to give cross-temporal reconciliation, all whilst producing density forecasts.

This talk will demonstrate the latest fable features using a real forecasting problem. In particular, cross-temporal reconciliation will be used to produce coherent density forecasts using data observed at mixed temporal granularities.

#### Notes:

Fable builds on tsibble (also tsibbledata + feasts). A little intro to tsibble here.

Temporaral reconciliation uses package called moment (for temporal granularities)

Cross-temporal. Use `aggregate_key() %>% aggregate_index()`. 

https://slides.mitchelloharawild.com/isf2020/

### A tool to detect potential data leaks in forecasting competitions
#### Thiyanga Talagala
#### Abstract:
Forecasting competitions are of increasing importance as a mean to learn best practices and gain knowledge. Data leakage is one of the most common issues that can often be found in competitions. Data leaks can happen when the training data contains information about the test data. There are a variety of different ways that data leaks can occur with time series data. For example: i) randomly chosen blocks of time series are concatenated to form a new time series, ii) scale-shifts, iii) repeating patterns in time series, iv) white noise is added in the original time series to form a new time series, etc. This work introduces a novel tool to detect these data leaks. The tsdataleaks package provides simple and computationally efficient algorithm to exploit data leaks in time series data. I will demonstrate the package design and its power to detect data leakages using recent forecasting competitions data.

#### Notes:
New R package: tsdataleaks

Data leakage if one series is subset of another series. Examples of this from M1 and M4 competition.

Algorithm to detect data leakage based on pearson's correlation. Takes window of series and slides across other series to calculate correlations, looking for periods of very high correlations. Implemented in tsdataleaks.

Main function is `find_dataleaks()`. A bit of explanation about how to use it. Can also determine reason in `reason_dataleaks()`.

She didn't check M5 as package not fast enough yet.


# Practicioner session: Enhancing shipment forecast for CPG companies using machine learning and demand sensing

### Varun Valsaraj (SAS AI Center of Excellence), Kedar Prabhudesai (SAS AI Center of Excellence)

#### Notes:

Forecasting shipments for CPG companies

Challenge: Agile to shifting market conditions, changing consumer expectations etc. Need to leverage real-time demand signals. Use machine learning to improve short-term forecast. Automatically integrate results into planning and execution systems.

Signals:
- Order/shipment history
- Future orders (usually a few days in advance, but sometimes up to a month)
- POS and customer inventory

Feed to ensemble of ML and classical models to get weekly forecasts, then broken down into daily forecast.

Benefits:
- Increase revenue
- Reduce costs
- Improve customer service
- Consumer satisfaction

Weekly forecast Method:
Input Data > Pre-process > Hierarchical time-series forecasting + ML forecasting > champion model > enhanced weekly forecast

Daily forecast method:
Enhanced weekly forecast > seasonal model, trend model, machine learning model > champion model > enhanced daily forecast

ML weekly forecast:
data > time-series forecast > NN(trend+predictions, product hierarchy, 1,2,3,4-year order lags): single hidden layer > time-series forecast on residuals, OR nothing > Forecast

Data used for forecast comparison:
- FC-Base forecast (base forecast that the CPG company uses, classical only)
- FC-Base+Expert forecast (manually tweaked forecasts)

Input data:
- order history, pos, inventory (last 3 years)
- future orders

Split data into training/val/testing. Use validation data to choose the best model whenever there is a choice to make about that.


Metrics:
CPG company currently uses these metrics, so SAS uses the same metrics.

Accuracy $$\left[1 - \frac{\sum_i |F_i - O_i|}{\sum F_i}\right] \times 100$$
Bias $$\left[ 1 - \frac{\sum_i O_i}{\sum_i F_i}\right] \times 100 $$

##### Weekly forecast accuracy

Expert overrides help in short term (<= 4 weeks), but hinder in long term. The SAS forecasts are much better (+ ~6%) across all 8 lag periods.

Bias metric is also better. CPG bias tends to be under forecasting. SAS is close to zero in lags 1-4, and positive for weeks 5-8.

##### POS data
Measured effect on accuracy of including POS data. Including POS + Inventory data increased accuracy by about 1%.

Why not directly forecast at daily level?
- very noisy
- aggregating to weekly reduces these fluctuations

Three methods are tried

enhanced weekly forecast, order shipment history/future orders/pos data > long term seasonal model, short term trend model, machine learning model > champion model > get forecasts

##### Short term trend model
Look at daily proportions over last few weeks. Get average proportions then multiply out by weekly forecast. (have to be careful with outliers though)

##### Long term seasonal model
Look at long term three year period. Look at proportions for each year then multiply out by weekly forecasts.

##### Machine learning based model
NN that takes proportions from last 2 years and outputs proportions for now. Then multiply by weekly forecast.

Shows that enhanced daily forecast is significantly more accurate (+30% accuracy) than the naive daily forecast (evenly distribute weekly forecast into days). Bias improves too. Enhanced forecasts remove a lot of bias and are generally close to 0.

Nicely detailed talk :) 

Q+A: 
- Worked with 33,000 time series
- Analysis + training time, then 2.5 to 3 hours (not sure if this includes training time), use high performance engines in SAS.
- Did you have to handle new times series without past data? if so, how do you do this? In preprocessing stage did segmentation of time series. Some series did not have enough history - for these, just used classical models. [someone in comments suggested this is the ideal situation for global ml models]

# Keynote: Arnold Zellner Memorial
#### Andrew Harvey

New class of time series models for predicting a variable which when cumulated is subject to an unknown saturation level. Very successful for forecasting covid cases in germany.

These models are estimated by MLE.

Not a compartmental model. Uses historical data to make predictions, rather than simulation.

$y_T$ is daily number of new cases
$Y_t$ is cumulative series, $y_t = Y_t - Y_{t-1}$. Don't model either of these directly. Instead the log of the growth rate of the cumulate series
$ln g_t$ where $$g_t = y_t / Y_{t-1}$$

That's just univariate time series, what about multivariate time series.

Gompertz growth curve. Family of growth curves called 'generalized logistic'. Gompertz curve when $\kappa -> \infty$. 

Didn't really follow this very well - but sounds like he had a good method for forecasting covid based on growth rates


# Practitioner session: Consolidated framework for forecast generation and evaluation

#### Chris Fry (Data Science Manager Google), Casey Lichtendahl (Visiting researcher google)

- Automated many steps of design generation and monitoring of time series forecasting
- use nested cross validation for generating and evaluating
- run jobs in parallel
- evaluate point and quantile forecasts with 30+ metrics in five categories at various horizons and levels of hierarchy

Tend to set capacity at 95 percentile.

Have lots of time series arranged in a hierarchy. e.g. Fleet > search, cloud, youtube > region > zone.

##### Nested cross validation
To evaluate, they use a sequence of backtests. 
To generate prediction intervals, use empirical error distributions from another sequence of backtests inside each training set

##### Parallelization
Thousands of time series, lots of backtests, lots of challenger models. Can parallelize over none, some, or all of these dimensions.

Use flume and beam. Similar to google's dataflow plus apache beam.

Inner loop
- Generate point forecasts (Series, backtests, models)
- Reconcile point forecasts and record forecasting errors
- Fit cones to empirical error distributions

Reconciliation not parallizable.

Reconcile with MinT or similar. Use in-sample fitted values to calculate projection matrix. Would be an issue for complex ML models... but issue for another time.

From each of your backtests, you get a bunch of 1-step, 2-step etc forecasts.. You can use prediction errors on the validation set to get empirical distributions for forecasts errors at each step.

##### detailed talks
Haoyun Wu - ml models
Weijie Shen - reconciliation
Kashif Yousuf... - empirical distributions for cones
Sercan Arik - temporal fusion ....

##### Evaluation metrics

- Accuracy (MSE)
- Bias (ME)
- Error variance (SDE)
- Accuracy risk (variability in an accuracy metric over multiple forecasts)
- Stability (degree to which forecasts remain unchanged subject to minor variations in the underlying data).

Example of two models with similar accuracy, but one is biased with low error variance (often can be fixed).

Put all of these metrics in a table with comparisons to standard models too (ETS + ARIMA).

Some example metrics
RMSE, MAE, MAPE, RMSLE WMAPE
ME, MPE, WMPE, MLE?
SE SPE WSPE
SAPE WSAPE
MPC WMPC
PPLq CRPS
WCoverage-q
accuracy risk WSPPL
stability MPC WMPC

You can monitor these metrics over time, and across forecast horizons, and across hierarchical levels.

His vision of where things are going at google:
classical time series models > increased reliance on ML models
borg/flume > tfx pipeline
python/c++ > tensorflow
post-process reconciliation > peri-process reconciliation
post-process empirical PIs > peri-process PIs


Open source tools

Tensorflow probability - bayesian structural time series
tensorflow - deep learning models
temporal fusion transformers
kubeflow for deploying and managing ML time series forecasting pipelines

BigQueryARIMA - forecast directly in SQL. 
AutoML tables - auto ml for structured data
demand AI - retail forecasting tool

##### Q+A

- Why use empirical PI instead of using pinball loss. They find that using pinball loss doesn't give very good distributions.
